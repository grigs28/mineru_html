# Copyright (c) Opendatalab. All rights reserved.

import base64
import os
import re
import time
import zipfile
import glob
import uuid
import asyncio
import json
import gc
import threading
from pathlib import Path
from datetime import datetime
from enum import Enum
from typing import Dict, Any

import click
import uvicorn
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, FileResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from starlette.background import BackgroundTask
from typing import List, Optional
from loguru import logger

# å°è¯•å¯¼å…¥MinerUæ¨¡å—ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ›¿ä»£å‡½æ•°
try:
    from mineru.cli.common import prepare_env, read_fn, aio_do_parse, pdf_suffixes, image_suffixes
    from mineru.utils.cli_parser import arg_parse
    from mineru.utils.hash_utils import str_sha256
    MINERU_AVAILABLE = True
except ImportError:
    # å¦‚æœMinerUæ¨¡å—ä¸å¯ç”¨ï¼Œåˆ›å»ºç®€å•çš„æ›¿ä»£å‡½æ•°
    MINERU_AVAILABLE = False
    pdf_suffixes = [".pdf"]
    image_suffixes = [".png", ".jpeg", ".jpg", ".webp", ".gif"]
    
    def prepare_env(output_dir, pdf_file_name, parse_method):
        local_md_dir = str(os.path.join(output_dir, pdf_file_name, parse_method))
        local_image_dir = os.path.join(str(local_md_dir), "images")
        os.makedirs(local_image_dir, exist_ok=True)
        os.makedirs(local_md_dir, exist_ok=True)
        return local_image_dir, local_md_dir
    
    def read_fn(path):
        if not isinstance(path, Path):
            path = Path(path)
        with open(str(path), "rb") as input_file:
            return input_file.read()
    
    async def aio_do_parse(*args, **kwargs):
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸è¿›è¡Œå®é™…å¤„ç†
        pass
    
    def arg_parse(ctx):
        return {}
    
    def str_sha256(text):
        import hashlib
        return hashlib.sha256(text.encode()).hexdigest()[:16]

# ä»»åŠ¡çŠ¶æ€æšä¸¾
class TaskStatus(Enum):
    PENDING = "pending"
    QUEUED = "queued"  # é˜Ÿåˆ—ä¸­
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

# é˜Ÿåˆ—çŠ¶æ€æšä¸¾
class QueueStatus(Enum):
    IDLE = "idle"          # ç©ºé—²
    RUNNING = "running"    # è¿è¡Œä¸­
    PAUSED = "paused"      # æš‚åœ

# ä»»åŠ¡ä¿¡æ¯ç±»
class TaskInfo:
    def __init__(self, task_id: str, filename: str, upload_time: datetime):
        self.task_id = task_id
        self.filename = filename
        self.upload_time = upload_time
        self.status = TaskStatus.PENDING
        self.progress = 0
        self.message = "ç­‰å¾…å¤„ç†"
        self.start_time = None
        self.end_time = None
        self.result_path = None
        self.error_message = None
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "filename": self.filename,
            "upload_time": self.upload_time.isoformat(),
            "status": self.status.value,
            "progress": self.progress,
            "message": self.message,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "result_path": self.result_path,
            "error_message": self.error_message
        }

# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨
class TaskManager:
    def __init__(self):
        self.tasks: Dict[str, TaskInfo] = {}
        self.task_storage_path = os.path.join("./config", "tasks.json")
        self.queue_status = QueueStatus.IDLE
        self.current_processing_task = None
        self.processing_lock = asyncio.Lock()
        self.load_tasks()
        self.load_queue_status()
        
    def create_task(self, filename: str) -> str:
        task_id = str(uuid.uuid4())
        task = TaskInfo(task_id, filename, datetime.now())
        self.tasks[task_id] = task
        self.save_tasks()
        return task_id
        
    def get_task(self, task_id: str) -> Optional[TaskInfo]:
        return self.tasks.get(task_id)
        
    def update_task_status(self, task_id: str, status: TaskStatus, progress: int = None, message: str = None, error_message: str = None):
        task = self.tasks.get(task_id)
        if task:
            task.status = status
            if progress is not None:
                task.progress = progress
            if message is not None:
                task.message = message
            if error_message is not None:
                task.error_message = error_message
            if status == TaskStatus.PROCESSING and task.start_time is None:
                task.start_time = datetime.now()
                # çŠ¶æ€å˜ä¸ºPROCESSINGæ—¶åŒæ­¥åˆ°file_list.json
                self.sync_task_to_file_list(task)
            elif status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                # æˆåŠŸ/å¤±è´¥æ—¶è‹¥ç¼ºå°‘å¼€å§‹æ—¶é—´ï¼Œè¿›è¡Œå…œåº•ï¼šä¼˜å…ˆç”¨å·²æœ‰start_timeï¼Œå…¶æ¬¡ç”¨upload_timeï¼Œå†æ¬¡ç”¨å½“å‰æ—¶é—´
                if task.start_time is None:
                    task.start_time = task.upload_time or datetime.now()
                if not task.end_time:  # åªåœ¨ç¬¬ä¸€æ¬¡è®¾ç½®ç»“æŸæ—¶é—´
                    task.end_time = datetime.now()
                # ä»»åŠ¡å®Œæˆæ—¶åŒæ­¥åˆ° file_list.json
                self.sync_task_to_file_list(task)
            elif status == TaskStatus.QUEUED:
                # çŠ¶æ€å˜ä¸ºQUEUEDæ—¶ä¹ŸåŒæ­¥åˆ°file_list.json
                self.sync_task_to_file_list(task)
            self.save_tasks()
            
    def get_all_tasks(self) -> List[Dict[str, Any]]:
        return [task.to_dict() for task in self.tasks.values()]
    
    def sync_task_to_file_list(self, task):
        """å°†ä»»åŠ¡ä¿¡æ¯åŒæ­¥åˆ° file_list.json"""
        try:
            # è·å–å½“å‰æ–‡ä»¶åˆ—è¡¨
            current_file_list = load_server_file_list()
            
            # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡ä»¶
            file_found = False
            for file_info in current_file_list:
                if file_info.get("taskId") == task.task_id:
                    # æ›´æ–°ç°æœ‰æ–‡ä»¶ä¿¡æ¯
                    file_info.update({
                        "status": task.status.value,
                        "progress": task.progress,
                        "message": task.message,
                        "startTime": task.start_time.isoformat() if task.start_time else None,
                        "endTime": task.end_time.isoformat() if task.end_time else None,
                        "processingTime": (task.end_time - task.start_time).total_seconds() if task.start_time and task.end_time else None,
                        "errorMessage": task.error_message,
                        "outputDir": task.result_path
                    })
                    file_found = True
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ·»åŠ æ–°æ–‡ä»¶ä¿¡æ¯
            if not file_found:
                new_file_info = {
                    "name": task.filename,
                    "size": 0,  # æ–‡ä»¶å¤§å°ä¿¡æ¯å¯èƒ½ä¸¢å¤±
                    "status": task.status.value,
                    "uploadTime": task.upload_time.isoformat() if task.upload_time else None,
                    "startTime": task.start_time.isoformat() if task.start_time else None,
                    "endTime": task.end_time.isoformat() if task.end_time else None,
                    "processingTime": (task.end_time - task.start_time).total_seconds() if task.start_time and task.end_time else None,
                    "taskId": task.task_id,
                    "progress": task.progress,
                    "message": task.message,
                    "errorMessage": task.error_message,
                    "outputDir": task.result_path
                }
                current_file_list.append(new_file_info)
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶åˆ—è¡¨
            save_server_file_list(current_file_list)
            logger.info(f"ä»»åŠ¡ {task.task_id} å·²åŒæ­¥åˆ° file_list.json")
            
        except Exception as e:
            logger.warning(f"åŒæ­¥ä»»åŠ¡åˆ° file_list.json å¤±è´¥: {e}")
        
    def save_tasks(self):
        try:
            _ensure_config_dir()
            with open(self.task_storage_path, 'w', encoding='utf-8') as f:
                json.dump(self.get_all_tasks(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ä¿å­˜ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            
    def load_tasks(self):
        try:
            _ensure_config_dir()
            if os.path.exists(self.task_storage_path):
                with open(self.task_storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for task_data in data:
                        task = TaskInfo(task_data["task_id"], task_data["filename"], 
                                      datetime.fromisoformat(task_data["upload_time"]))
                        task.status = TaskStatus(task_data["status"])
                        task.progress = task_data.get("progress", 0)
                        task.message = task_data.get("message", "ç­‰å¾…å¤„ç†")
                        task.start_time = datetime.fromisoformat(task_data["start_time"]) if task_data.get("start_time") else None
                        task.end_time = datetime.fromisoformat(task_data["end_time"]) if task_data.get("end_time") else None
                        task.result_path = task_data.get("result_path")
                        task.error_message = task_data.get("error_message")
                        self.tasks[task_data["task_id"]] = task
        except Exception as e:
            logger.warning(f"åŠ è½½ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
    
    def load_queue_status(self):
        """åŠ è½½é˜Ÿåˆ—çŠ¶æ€"""
        try:
            _ensure_config_dir()
            queue_status_path = os.path.join("./config", "queue_status.json")
            if os.path.exists(queue_status_path):
                with open(queue_status_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.queue_status = QueueStatus(data.get("status", "idle"))
                    self.current_processing_task = data.get("current_processing_task")
        except Exception as e:
            logger.warning(f"åŠ è½½é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
    
    def save_queue_status(self):
        """ä¿å­˜é˜Ÿåˆ—çŠ¶æ€"""
        try:
            _ensure_config_dir()
            queue_status_path = os.path.join("./config", "queue_status.json")
            with open(queue_status_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "status": self.queue_status.value,
                    "current_processing_task": self.current_processing_task
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ä¿å­˜é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
    
    def get_queue_tasks(self) -> List[str]:
        """è·å–é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡IDåˆ—è¡¨"""
        queued_tasks = []
        for task_id, task in self.tasks.items():
            if task.status == TaskStatus.QUEUED:
                queued_tasks.append(task_id)
        # æŒ‰ä¸Šä¼ æ—¶é—´æ’åºï¼Œç¡®ä¿å…ˆè¿›å…ˆå‡º
        return sorted(queued_tasks, key=lambda tid: self.tasks[tid].upload_time)
    
    def get_next_task(self) -> Optional[str]:
        """è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ä»»åŠ¡ID"""
        queued_tasks = self.get_queue_tasks()
        if queued_tasks:
            return queued_tasks[0]
        return None
    
    def start_queue(self):
        """å¯åŠ¨é˜Ÿåˆ—å¤„ç†"""
        if self.queue_status == QueueStatus.IDLE:
            self.queue_status = QueueStatus.RUNNING
            self.save_queue_status()
            logger.info("ä»»åŠ¡é˜Ÿåˆ—å·²å¯åŠ¨")
    
    def stop_queue(self):
        """åœæ­¢é˜Ÿåˆ—å¤„ç†"""
        self.queue_status = QueueStatus.IDLE
        self.current_processing_task = None
        self.save_queue_status()
        logger.info("ä»»åŠ¡é˜Ÿåˆ—å·²åœæ­¢")
    
    def add_to_queue(self, task_id: str):
        """å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—"""
        task = self.tasks.get(task_id)
        if task and task.status == TaskStatus.PENDING:
            task.status = TaskStatus.QUEUED
            task.message = "å·²åŠ å…¥é˜Ÿåˆ—"
            # å¼€å§‹æ—¶é—´åº”åœ¨è¿›å…¥ PROCESSING æ—¶è®¾ç½®ï¼Œè¿™é‡Œä¸è®¾ç½®
            self.save_tasks()
            logger.info(f"ä»»åŠ¡ {task_id} å·²åŠ å…¥é˜Ÿåˆ—")
            
            # å¦‚æœé˜Ÿåˆ—ç©ºé—²ï¼Œå¯åŠ¨é˜Ÿåˆ—ï¼ˆé¿å…é‡å¤å¯åŠ¨ï¼‰
            if self.queue_status == QueueStatus.IDLE:
                self.start_queue()
                asyncio.create_task(self.process_queue())
    
    async def process_queue(self):
        """å¤„ç†é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
        async with self.processing_lock:
            while self.queue_status == QueueStatus.RUNNING:
                next_task_id = self.get_next_task()
                if not next_task_id:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…æ–°ä»»åŠ¡è€Œä¸æ˜¯åœæ­¢é˜Ÿåˆ—
                    await asyncio.sleep(1)
                    continue
                
                self.current_processing_task = next_task_id
                self.save_queue_status()
                
                try:
                    await self.process_single_task(next_task_id)
                except Exception as e:
                    logger.error(f"å¤„ç†ä»»åŠ¡ {next_task_id} å¤±è´¥: {e}")
                    self.update_task_status(next_task_id, TaskStatus.FAILED, 0, "å¤„ç†å¤±è´¥", str(e))
                finally:
                    # å¤„ç†å®Œæˆåç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼Œæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥
                    self.current_processing_task = None
                    self.save_queue_status()
                    # ä»»åŠ¡å®Œæˆåæ¸…ç†æ˜¾å­˜
                    cleanup_vram()
                    # ç»§ç»­å¤„ç†é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼Œå³ä½¿å½“å‰ä»»åŠ¡å¤±è´¥
                    pass
    
    async def process_single_task(self, task_id: str):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task = self.tasks.get(task_id)
        if not task:
            return
            
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        self.update_task_status(task_id, TaskStatus.PROCESSING, 20, "å¼€å§‹å¤„ç†æ–‡ä»¶")
        await asyncio.sleep(0.5)  # è®©çŠ¶æ€å˜åŒ–å¯è§
        
        # æŸ¥æ‰¾ä¸Šä¼ çš„æ–‡ä»¶
        output_dir = "./output"
        uploaded_file = None
        for filename in os.listdir(output_dir):
            if filename.startswith(f"{task_id}_"):
                uploaded_file = os.path.join(output_dir, filename)
                break
                
        if not uploaded_file:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯æµ‹è¯•ç¯å¢ƒï¼Œæ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
            self.update_task_status(task_id, TaskStatus.PROCESSING, 30, "æ­£åœ¨è§£ææ–‡ä»¶")
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            self.update_task_status(task_id, TaskStatus.PROCESSING, 50, "æ­£åœ¨å¤„ç†æ–‡ä»¶å†…å®¹")
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            self.update_task_status(task_id, TaskStatus.PROCESSING, 80, "å¤„ç†å®Œæˆï¼Œç”Ÿæˆç»“æœæ–‡ä»¶")
            await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            self.update_task_status(task_id, TaskStatus.COMPLETED, 100, "è½¬æ¢å®Œæˆ", None)
            logger.info(f"ä»»åŠ¡ {task_id} å¤„ç†å®Œæˆï¼ˆæ¨¡æ‹Ÿï¼‰")
            return
            
        # æ£€æŸ¥æ˜¾å­˜æ˜¯å¦å¯ç”¨
        if not check_vram_available():
            self.update_task_status(task_id, TaskStatus.FAILED, 0, "å¤„ç†å¤±è´¥", "æ˜¾å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç†æ–‡ä»¶")
            logger.error(f"ä»»åŠ¡ {task_id} å¤±è´¥ï¼šæ˜¾å­˜ä¸è¶³")
            cleanup_vram()  # å°è¯•æ¸…ç†æ˜¾å­˜
            return
        
        # å¼€å§‹å¤„ç†
        self.update_task_status(task_id, TaskStatus.PROCESSING, 30, "æ­£åœ¨è§£ææ–‡ä»¶")
        
        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        async def update_progress(progress, message):
            self.update_task_status(task_id, TaskStatus.PROCESSING, progress, message)
            # æ·»åŠ æ—¥å¿—è®°å½•è¿›åº¦æ›´æ–°
            logger.info(f"ä»»åŠ¡ {task_id} è¿›åº¦æ›´æ–°: {progress}% - {message}")
        
        # ä½¿ç”¨ç°æœ‰çš„parse_pdfå‡½æ•°è¿›è¡Œå¤„ç†
        result = await parse_pdf(
            doc_path=uploaded_file,
            output_dir=output_dir,
            end_page_id=99999,
            is_ocr=False,
            formula_enable=True,
            table_enable=True,
            language="ch",
            backend="vlm-sglang-engine",
            url=None,
            progress_callback=update_progress
        )
        
        if result:
            local_md_dir, file_name = result
            self.update_task_status(task_id, TaskStatus.PROCESSING, 80, "å¤„ç†å®Œæˆï¼Œç”Ÿæˆç»“æœæ–‡ä»¶")
            
            # ä¿å­˜ç»“æœè·¯å¾„
            task.result_path = local_md_dir
            self.update_task_status(task_id, TaskStatus.COMPLETED, 100, "è½¬æ¢å®Œæˆ", None)
            
            # è¾“å‡ºoutputç›®å½•ä¿¡æ¯
            print(f"âœ… æ–‡ä»¶è½¬æ¢æˆåŠŸ: {file_name}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {local_md_dir}")
            logger.info(f"ä»»åŠ¡ {task_id} å¤„ç†å®Œæˆ: {file_name}")
            logger.info(f"è¾“å‡ºç›®å½•: {local_md_dir}")
            
            # æ¸…ç†ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶
            try:
                os.remove(uploaded_file)
            except:
                pass
        else:
            self.update_task_status(task_id, TaskStatus.FAILED, 0, "å¤„ç†å¤±è´¥", "è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        
        # ä»»åŠ¡å®Œæˆåæ¸…ç†æ˜¾å­˜
        cleanup_vram()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="MinerU Web Interface", version="0.1.8")
app.add_middleware(GZipMiddleware, minimum_size=1000)

# åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
task_manager = TaskManager()

# è·å–é™æ€æ–‡ä»¶ç›®å½•è·¯å¾„
static_dir = os.path.join(os.path.dirname(__file__), "static")
if not os.path.exists(static_dir):
    os.makedirs(static_dir, exist_ok=True)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# æœåŠ¡å™¨ç«¯æ–‡ä»¶åˆ—è¡¨å­˜å‚¨ï¼ˆä½¿ç”¨ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œé¿å…å·¥ä½œç›®å½•å·®å¼‚å½±å“ï¼‰
BASE_DIR = os.path.dirname(__file__)
CONFIG_DIR = os.path.join(BASE_DIR, "config")
FILE_LIST_PATH = os.path.join(CONFIG_DIR, "file_list.json")

# æ–‡ä»¶é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
file_list_lock = threading.Lock()

def _ensure_output_dir():
    os.makedirs("./output", exist_ok=True)

def _ensure_config_dir():
    os.makedirs(CONFIG_DIR, exist_ok=True)

def load_server_file_list() -> list:
    _ensure_config_dir()
    with file_list_lock:
        if os.path.exists(FILE_LIST_PATH):
            try:
                import json
                with open(FILE_LIST_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return data
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return []

def save_server_file_list(file_list: list) -> None:
    _ensure_config_dir()
    with file_list_lock:
        try:
            import json
            with open(FILE_LIST_PATH, 'w', encoding='utf-8') as f:
                json.dump(file_list, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"å†™å…¥æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")

def sanitize_filename(filename: str) -> str:
    """æ ¼å¼åŒ–å‹ç¼©æ–‡ä»¶çš„æ–‡ä»¶å"""
    sanitized = re.sub(r'[/\\\.]{2,}|[/\\]', '', filename)
    sanitized = re.sub(r'[^\w.-]', '_', sanitized, flags=re.UNICODE)
    if sanitized.startswith('.'):
        sanitized = '_' + sanitized[1:]
    return sanitized or 'unnamed'

def image_to_base64(image_path: str) -> str:
    """å°†å›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸ºbase64ç¼–ç """
    try:
        with open(image_path, 'rb') as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        logger.error(f"è½¬æ¢å›¾ç‰‡ä¸ºbase64å¤±è´¥: {e}")
        return ""

def replace_image_with_base64(markdown_text: str, image_dir_path: str) -> str:
    """å°†Markdownä¸­çš„å›¾ç‰‡è·¯å¾„æ›¿æ¢ä¸ºbase64ç¼–ç """
    # åŒ¹é…Markdownä¸­çš„å›¾ç‰‡æ ‡ç­¾
    pattern = r'\!\[(?:[^\]]*)\]\(([^)]+)\)'
    
    def replace(match):
        relative_path = match.group(1)
        full_path = os.path.join(image_dir_path, relative_path)
        if os.path.exists(full_path):
            base64_image = image_to_base64(full_path)
            # ä¿æŒåŸå§‹çš„altæ–‡æœ¬ï¼Œåªæ›¿æ¢URLéƒ¨åˆ†
            return match.group(0).replace(f'({relative_path})', f'(data:image/jpeg;base64,{base64_image})')
        else:
            # å¦‚æœå›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›åŸå§‹é“¾æ¥
            return match.group(0)
    
    # åº”ç”¨æ›¿æ¢
    return re.sub(pattern, replace, markdown_text)

def cleanup_file(file_path: str) -> None:
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
    except Exception as e:
        logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

async def load_task_markdown_content(filename: str, result_path: str) -> tuple[str, str]:
    """åŠ è½½ä»»åŠ¡çš„Markdownå†…å®¹"""
    try:
        if not result_path or not os.path.exists(result_path):
            return "", ""
        
        # æŸ¥æ‰¾Markdownæ–‡ä»¶
        md_files = []
        for root, dirs, files in os.walk(result_path):
            for file in files:
                if file.endswith('.md'):
                    md_files.append(os.path.join(root, file))
        
        if not md_files:
            logger.warning(f"No markdown files found in: {result_path}")
            return "", ""
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„Markdownæ–‡ä»¶
        md_path = md_files[0]
        logger.info(f"Loading markdown file: {md_path}")
        
        with open(md_path, 'r', encoding='utf-8') as f:
            txt_content = f.read()
        
        # è½¬æ¢å›¾ç‰‡ä¸ºbase64
        md_content = replace_image_with_base64(txt_content, result_path)
        
        logger.info(f"Successfully loaded markdown content, length: {len(md_content)}")
        return md_content, txt_content
        
    except Exception as e:
        logger.error(f"åŠ è½½Markdownå†…å®¹å¤±è´¥: {e}")
        return "", ""

def safe_stem(file_path):
    """å®‰å…¨åœ°è·å–æ–‡ä»¶åçš„steméƒ¨åˆ†"""
    stem = Path(file_path).stem
    # åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€ç‚¹å’Œä¸­æ–‡å­—ç¬¦ï¼Œå…¶ä»–å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    return re.sub(r'[^\w.\u4e00-\u9fff]', '_', stem)

def cleanup_vram():
    """æ¸…ç†æ˜¾å­˜"""
    gc.collect()
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.info("æ˜¾å­˜æ¸…ç†å®Œæˆ")
    except ImportError:
        logger.info("PyTorchæœªå®‰è£…ï¼Œè·³è¿‡æ˜¾å­˜æ¸…ç†")

def check_vram_available():
    """æ£€æŸ¥æ˜¾å­˜æ˜¯å¦å¯ç”¨"""
    try:
        import torch
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory
            allocated_memory = torch.cuda.memory_allocated(0)
            free_memory = total_memory - allocated_memory
            
            # å•GPUç¯å¢ƒï¼Œéœ€è¦è‡³å°‘1.5GBæ˜¾å­˜æ‰èƒ½å¤„ç†
            required_memory = 1.5 * 1024**3
            is_available = free_memory > required_memory
            
            logger.info(f"æ˜¾å­˜çŠ¶æ€: æ€»è®¡={total_memory/1024**3:.1f}GB, "
                       f"å·²åˆ†é…={allocated_memory/1024**3:.1f}GB, "
                       f"å¯ç”¨={free_memory/1024**3:.1f}GB, "
                       f"å¯ç”¨æ€§={is_available}")
            
            return is_available
    except ImportError:
        logger.info("PyTorchæœªå®‰è£…ï¼Œå‡è®¾æ˜¾å­˜å¯ç”¨")
    
    return True  # CPUæ¨¡å¼æ€»æ˜¯å¯ç”¨

def to_pdf(file_path):
    """å°†æ–‡ä»¶è½¬æ¢ä¸ºPDFæ ¼å¼"""
    if file_path is None:
        return None

    pdf_bytes = read_fn(file_path)
    unique_filename = f'{safe_stem(file_path)}.pdf'
    
    # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    tmp_file_path = os.path.join(os.path.dirname(file_path), unique_filename)
    
    # å°†å­—èŠ‚æ•°æ®å†™å…¥æ–‡ä»¶
    with open(tmp_file_path, 'wb') as tmp_pdf_file:
        tmp_pdf_file.write(pdf_bytes)
    
    return tmp_file_path

async def parse_pdf(doc_path, output_dir, end_page_id, is_ocr, formula_enable, table_enable, language, backend, url, progress_callback=None):
    """è§£æPDFæ–‡ä»¶ï¼Œé‡‡ç”¨ä¸sampleæ–‡ä»¶ç›¸åŒçš„è½¬æ¢æ–¹æ³•"""
    os.makedirs(output_dir, exist_ok=True)

    try:
        file_name = f'{safe_stem(Path(doc_path).stem)}_{time.strftime("%y%m%d_%H%M%S")}'
        pdf_data = read_fn(doc_path)
        if is_ocr:
            parse_method = 'ocr'
        else:
            parse_method = 'auto'

        if backend.startswith("vlm"):
            parse_method = "vlm"

        local_image_dir, local_md_dir = prepare_env(output_dir, file_name, parse_method)
        
        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹å¤„ç†
        if progress_callback:
            await progress_callback(40, "å¼€å§‹è§£æPDFå†…å®¹")
            await asyncio.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿè®©è¿›åº¦æ›´æ–°å¯è§
        
        # æ›´æ–°è¿›åº¦ï¼šæ­£åœ¨å¤„ç†
        if progress_callback:
            await progress_callback(50, "æ­£åœ¨å¤„ç†PDFå†…å®¹ï¼Œè¯·ç¨å€™...")
            await asyncio.sleep(0.1)
        
        # å¯åŠ¨è¿›åº¦æ¨¡æ‹Ÿå™¨
        async def progress_simulator():
            """æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°ï¼Œè®©ç”¨æˆ·çœ‹åˆ°å¤„ç†æ­£åœ¨è¿›è¡Œ"""
            progress = 50
            while progress <= 64:  # æœ€å¤šåˆ°64%
                await asyncio.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
                progress += 2
                if progress_callback and progress <= 64:  # ç¡®ä¿ä¸è¶…è¿‡64%
                    await progress_callback(progress, f"æ­£åœ¨å¤„ç†PDFå†…å®¹... ({progress}%)")
        
        # åŒæ—¶è¿è¡Œå¤„ç†ä»»åŠ¡å’Œè¿›åº¦æ¨¡æ‹Ÿå™¨
        if progress_callback:
            # åˆ›å»ºè¿›åº¦æ¨¡æ‹Ÿä»»åŠ¡
            progress_task = asyncio.create_task(progress_simulator())
            
            # è¿è¡Œå®é™…å¤„ç†ä»»åŠ¡
            parse_task = asyncio.create_task(aio_do_parse(
                output_dir=output_dir,
                pdf_file_names=[file_name],
                pdf_bytes_list=[pdf_data],
                p_lang_list=[language],
                parse_method=parse_method,
                end_page_id=end_page_id,
                formula_enable=formula_enable,
                table_enable=table_enable,
                backend=backend,
                server_url=url,
            ))
            
            # ç­‰å¾…å¤„ç†å®Œæˆ
            await parse_task
            
            # å–æ¶ˆè¿›åº¦æ¨¡æ‹Ÿå™¨
            progress_task.cancel()
            try:
                await progress_task
            except asyncio.CancelledError:
                pass
        else:
            # å¦‚æœæ²¡æœ‰è¿›åº¦å›è°ƒï¼Œç›´æ¥è¿è¡Œå¤„ç†
            await aio_do_parse(
                output_dir=output_dir,
                pdf_file_names=[file_name],
                pdf_bytes_list=[pdf_data],
                p_lang_list=[language],
                parse_method=parse_method,
                end_page_id=end_page_id,
                formula_enable=formula_enable,
                table_enable=table_enable,
                backend=backend,
                server_url=url,
            )
        
        # æ›´æ–°è¿›åº¦ï¼šå¤„ç†å®Œæˆ
        if progress_callback:
            await progress_callback(70, "PDFè§£æå®Œæˆï¼Œç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
        
        return local_md_dir, file_name
    except Exception as e:
        logger.exception(e)
        return None

@app.get("/api/backend_options")
async def get_backend_options():
    """è·å–å¯ç”¨çš„åç«¯é€‰é¡¹"""
    try:
        sglang_engine_enable = getattr(app.state, 'sglang_engine_enable', False)
        
        if sglang_engine_enable:
            backend_options = [
                {"value": "pipeline", "label": "Pipeline"},
                {"value": "vlm-sglang-engine", "label": "VLM SgLang Engine"}
            ]
            default_backend = "vlm-sglang-engine"
        else:
            backend_options = [
                {"value": "pipeline", "label": "Pipeline"},
                {"value": "vlm-transformers", "label": "VLM Transformers"},
                {"value": "vlm-sglang-client", "label": "VLM SgLang Client"},
                {"value": "vlm-sglang-engine", "label": "VLM SgLang Engine"}
            ]
            default_backend = "vlm-sglang-engine"
        
        return JSONResponse(content={
            "backend_options": backend_options,
            "default_backend": default_backend
        })
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"è·å–åç«¯é€‰é¡¹å¤±è´¥: {str(e)}"}
        )

@app.get("/CHANGELOG.md")
async def get_changelog():
    """è·å–CHANGELOG.mdæ–‡ä»¶å†…å®¹"""
    try:
        changelog_path = os.path.join(os.path.dirname(__file__), "CHANGELOG.md")
        if os.path.exists(changelog_path):
            with open(changelog_path, "r", encoding="utf-8") as f:
                content = f.read()
            return HTMLResponse(content=content, media_type="text/plain; charset=utf-8")
        else:
            return JSONResponse(
                status_code=404,
                content={"error": "CHANGELOG.mdæ–‡ä»¶æœªæ‰¾åˆ°"}
            )
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"è¯»å–CHANGELOGå¤±è´¥: {str(e)}"}
        )

@app.get("/api/version")
async def get_version():
    """è¿”å›æœ€æ–°ç‰ˆæœ¬å·ï¼Œè§£æ CHANGELOG.md ç¬¬ä¸€æ¡ç‰ˆæœ¬è®°å½•ã€‚"""
    try:
        changelog_path = os.path.join(os.path.dirname(__file__), "CHANGELOG.md")
        if os.path.exists(changelog_path):
            with open(changelog_path, "r", encoding="utf-8") as f:
                content = f.read()
            # æŸ¥æ‰¾å½¢å¦‚: ## [0.1.3] - yyyy-mm-dd çš„é¦–ä¸ªç‰ˆæœ¬
            m = re.search(r"^## \[(.*?)\]", content, flags=re.MULTILINE)
            if m and m.group(1):
                return JSONResponse(content={"version": f"v{m.group(1)}"})
        # å…œåº•
        return JSONResponse(content={"version": "v0.0.0"})
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"è¯»å–ç‰ˆæœ¬å¤±è´¥: {str(e)}"})

@app.get("/api/file_list")
async def api_get_file_list():
    """è·å–æœåŠ¡å™¨ç«¯å…±äº«çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºå¤šPCå…±äº«ï¼‰ã€‚"""
    try:
        # ä¼˜å…ˆä»file_list.jsonè·å–æ–‡ä»¶åˆ—è¡¨
        file_list = load_server_file_list()
        
        # å¦‚æœfile_list.jsonä¸ºç©ºï¼Œåˆ™ä»ä»»åŠ¡ç®¡ç†å™¨è·å–
        if not file_list:
            for task in task_manager.tasks.values():
                file_info = {
                    "name": task.filename,
                    "size": 0,  # æ–‡ä»¶å¤§å°ä¿¡æ¯å¯èƒ½ä¸¢å¤±
                    "status": task.status.value,
                    "uploadTime": task.upload_time.isoformat() if task.upload_time else None,
                    "startTime": task.start_time.isoformat() if task.start_time else None,
                    "endTime": task.end_time.isoformat() if task.end_time else None,
                    "processingTime": None,
                    "taskId": task.task_id,
                    "progress": task.progress,
                    "message": task.message,
                    "errorMessage": task.error_message,
                    "outputDir": task.result_path or None  # æ·»åŠ è¾“å‡ºç›®å½•å­—æ®µ
                }
                
                # è®¡ç®—å¤„ç†æ—¶é—´
                if task.start_time and task.end_time:
                    duration = (task.end_time - task.start_time).total_seconds()
                    file_info["processingTime"] = duration
                
                file_list.append(file_info)
        
        # æŒ‰ä¸Šä¼ æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        file_list.sort(key=lambda x: x.get("uploadTime") or "", reverse=True)
        
        return JSONResponse(content=file_list)
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"})

@app.post("/api/file_list")
async def api_set_file_list(payload: dict):
    """è®¾ç½®ï¼ˆè¦†ç›–ï¼‰æœåŠ¡å™¨ç«¯æ–‡ä»¶åˆ—è¡¨ã€‚payload: {"files": [...]}"""
    try:
        files = payload.get("files", [])
        if not isinstance(files, list):
            return JSONResponse(status_code=400, content={"error": "fileså¿…é¡»æ˜¯æ•°ç»„"})
        save_server_file_list(files)
        return JSONResponse(content={"ok": True})
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"ä¿å­˜æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"})

@app.post("/api/remove_file")
async def api_remove_file(request: dict):
    """åˆ é™¤å•ä¸ªæ–‡ä»¶"""
    try:
        filename = request.get("filename")
        if not filename:
            return JSONResponse(status_code=400, content={"error": "ç¼ºå°‘æ–‡ä»¶å"})
        
        # ä»æ–‡ä»¶åˆ—è¡¨ä¸­è·å–taskIdå¹¶åˆ é™¤è®°å½•
        current_file_list = load_server_file_list()
        task_to_remove = None
        updated_file_list = []
        
        for f in current_file_list:
            if f.get("name") == filename:
                task_to_remove = f.get("taskId")
                logger.info(f"æ‰¾åˆ°è¦åˆ é™¤çš„æ–‡ä»¶: {filename}, taskId: {task_to_remove}")
            else:
                updated_file_list.append(f)
        
        # åˆ é™¤å¯¹åº”çš„è¾“å‡ºç›®å½•
        if task_to_remove:
            output_dir = "./output"
            if os.path.exists(output_dir):
                for dir_name in os.listdir(output_dir):
                    if dir_name.startswith(task_to_remove.replace('-', '_')):
                        dir_path = os.path.join(output_dir, dir_name)
                        if os.path.isdir(dir_path):
                            import shutil
                            shutil.rmtree(dir_path)
                            logger.info(f"å·²åˆ é™¤è¾“å‡ºç›®å½•: {dir_path}")
        
        # ä»ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ é™¤å¯¹åº”çš„ä»»åŠ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if task_to_remove and task_to_remove in task_manager.tasks:
            del task_manager.tasks[task_to_remove]
            task_manager.save_tasks()
            logger.info(f"å·²ä»ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ é™¤ä»»åŠ¡: {task_to_remove}")
        
        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶åˆ—è¡¨
        save_server_file_list(updated_file_list)
        
        logger.info(f"æ–‡ä»¶ {filename} å·²ä»åˆ—è¡¨ã€ä»»åŠ¡å’Œè¾“å‡ºç›®å½•ä¸­åˆ é™¤")
        return JSONResponse(content={"ok": True, "message": f"æ–‡ä»¶ {filename} å·²åˆ é™¤"})
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"})

@app.post("/api/clear_all")
async def api_clear_all():
    """æ¸…ç©ºæ‰€æœ‰ä»»åŠ¡å’Œæ–‡ä»¶åˆ—è¡¨"""
    try:
        # æ¸…ç©ºä»»åŠ¡ç®¡ç†å™¨
        task_manager.tasks.clear()
        task_manager.current_processing_task = None
        task_manager.queue_status = QueueStatus.IDLE
        task_manager.save_tasks()
        task_manager.save_queue_status()
        
        # æ¸…ç©ºæœåŠ¡å™¨æ–‡ä»¶åˆ—è¡¨
        save_server_file_list([])
        
        logger.info("æ‰€æœ‰ä»»åŠ¡å’Œæ–‡ä»¶åˆ—è¡¨å·²æ¸…ç©º")
        return JSONResponse(content={"ok": True, "message": "æ‰€æœ‰ä»»åŠ¡å·²æ¸…ç©º"})
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"æ¸…ç©ºå¤±è´¥: {str(e)}"})

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """è¿”å›ä¸»é¡µé¢"""
    html_path = os.path.join(static_dir, "index.html")
    if os.path.exists(html_path):
        with open(html_path, "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    else:
        # è¿”å›é”™è¯¯é¡µé¢
        return HTMLResponse(content="""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MinerU PDFè½¬æ¢å·¥å…·</title>
</head>
<body>
            <h1>MinerU PDFè½¬æ¢å·¥å…·</h1>
    <p>é™æ€æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥static/index.htmlæ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚</p>
</body>
</html>
        """)

@app.post("/file_parse")
async def parse_files(
    files: List[UploadFile] = File(...),
    output_dir: str = Form("./output"),
    lang_list: List[str] = Form(["ch"]),
    backend: str = Form("pipeline"),
    parse_method: str = Form("auto"),
    formula_enable: bool = Form(True),
    table_enable: bool = Form(True),
    server_url: Optional[str] = Form(None),
    return_md: bool = Form(True),
    return_images: bool = Form(True),
    response_format_zip: bool = Form(True),
    start_page_id: int = Form(0),
    end_page_id: int = Form(99999),
):
    """å¤„ç†æ–‡ä»¶è½¬æ¢"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        pdf_file_names = []
        pdf_bytes_list = []
        
        for file in files:
            content = await file.read()
            file_path = Path(file.filename)
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_path.suffix.lower() in pdf_suffixes + image_suffixes:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä»¥ä¾¿ä½¿ç”¨read_fn
                temp_path = Path(output_dir) / f"temp_{file_path.name}"
                with open(temp_path, "wb") as f:
                    f.write(content)
                
                try:
                    pdf_bytes = read_fn(temp_path)
                    pdf_bytes_list.append(pdf_bytes)
                    pdf_file_names.append(sanitize_filename(file_path.stem))
                    os.remove(temp_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                except Exception as e:
                    return JSONResponse(
                        status_code=400,
                        content={"error": f"åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}"}
                    )
            else:
                return JSONResponse(
                    status_code=400,
                    content={"error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}"}
                )
        
        # è®¾ç½®è¯­è¨€åˆ—è¡¨
        actual_lang_list = lang_list
        if len(actual_lang_list) != len(pdf_file_names):
            actual_lang_list = [actual_lang_list[0] if actual_lang_list else "ch"] * len(pdf_file_names)
        
        # å¦‚æœMinerUå¯ç”¨ï¼Œä½¿ç”¨ä¸sampleæ–‡ä»¶ç›¸åŒçš„è½¬æ¢æ–¹æ³•
        if MINERU_AVAILABLE:
            # ä½¿ç”¨æ–°çš„è½¬æ¢æ–¹æ³•ï¼Œä¸sampleæ–‡ä»¶ä¿æŒä¸€è‡´
            for i, (pdf_name, pdf_bytes) in enumerate(zip(pdf_file_names, pdf_bytes_list)):
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                temp_path = Path(output_dir) / f"temp_{pdf_name}.pdf"
                with open(temp_path, "wb") as f:
                    f.write(pdf_bytes)
                
                try:
                    # ä½¿ç”¨parse_pdfå‡½æ•°è¿›è¡Œè½¬æ¢
                    is_ocr = parse_method == 'ocr'
                    result = await parse_pdf(
                        doc_path=str(temp_path),
                        output_dir=output_dir,
                        end_page_id=end_page_id,
                        is_ocr=is_ocr,
                        formula_enable=formula_enable,
                        table_enable=table_enable,
                        language=actual_lang_list[i] if i < len(actual_lang_list) else actual_lang_list[0],
                        backend=backend,
                        url=server_url
                    )
                    
                    if result is None:
                        logger.error(f"è½¬æ¢æ–‡ä»¶å¤±è´¥: {pdf_name}")
                    
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_path.exists():
                        os.remove(temp_path)
        else:
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡ä»¶
            logger.info("ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬å¤„ç†æ–‡ä»¶")
        
        # åˆ›å»ºZIPæ–‡ä»¶
        import tempfile
        zip_fd, zip_path = tempfile.mkstemp(suffix=".zip", prefix="mineru_results_")
        os.close(zip_fd)
        
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            for pdf_name in pdf_file_names:
                safe_pdf_name = sanitize_filename(pdf_name)
                if backend.startswith("pipeline"):
                    parse_dir = os.path.join(output_dir, pdf_name, parse_method)
                else:
                    parse_dir = os.path.join(output_dir, pdf_name, "vlm")
                
                if not os.path.exists(parse_dir):
                    # åˆ›å»ºç¤ºä¾‹markdownæ–‡ä»¶
                    md_content = f"""# {pdf_name}

è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹Markdownæ–‡ä»¶ï¼Œç”±MinerU Webç•Œé¢ç”Ÿæˆã€‚

## æ–‡ä»¶ä¿¡æ¯
- æ–‡ä»¶å: {pdf_name}
- å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- åç«¯: {backend}
- è§£ææ–¹æ³•: {parse_method}

## è¯´æ˜
è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„MinerU Webç•Œé¢ï¼Œç”¨äºæ¼”ç¤ºåŸºæœ¬åŠŸèƒ½ã€‚
è¦ä½¿ç”¨å®Œæ•´çš„PDFè½¬æ¢åŠŸèƒ½ï¼Œè¯·ç¡®ä¿å®‰è£…äº†å®Œæ•´çš„MinerUç¯å¢ƒã€‚

## åŠŸèƒ½ç‰¹æ€§
- å¤šæ–‡ä»¶ä¸Šä¼ 
- æ–‡ä»¶ç±»å‹æ£€æŸ¥
- ZIPæ–‡ä»¶ç”Ÿæˆ
- ä¸­æ–‡ç•Œé¢æ”¯æŒ
"""
                    zf.writestr(f"{safe_pdf_name}/{safe_pdf_name}.md", md_content)
                else:
                    # å†™å…¥å®é™…çš„Markdownæ–‡ä»¶
                    if return_md:
                        md_path = os.path.join(parse_dir, f"{pdf_name}.md")
                        if os.path.exists(md_path):
                            zf.write(md_path, arcname=os.path.join(safe_pdf_name, f"{safe_pdf_name}.md"))
                    
                    # å†™å…¥å›¾ç‰‡
                    if return_images:
                        images_dir = os.path.join(parse_dir, "images")
                        if os.path.exists(images_dir):
                            image_paths = glob.glob(os.path.join(glob.escape(images_dir), "*.jpg"))
                            for image_path in image_paths:
                                zf.write(image_path, arcname=os.path.join(safe_pdf_name, "images", os.path.basename(image_path)))
        
        # æ ¹æ®å‚æ•°å†³å®šè¿”å›æ ¼å¼
        if response_format_zip:
            # è¿”å›ZIPæ–‡ä»¶
            return FileResponse(
                path=zip_path,
                media_type="application/zip",
                filename=f"{safe_pdf_name}.zip",
                background=BackgroundTask(cleanup_file, zip_path)
            )
        else:
            # è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«Markdownå†…å®¹
            md_content = ""
            txt_content = ""
            
            # å°è¯•è¯»å–Markdownæ–‡ä»¶å†…å®¹
            for pdf_name in pdf_file_names:
                # ä½¿ç”¨åŸå§‹æ–‡ä»¶åè¿›è¡ŒåŒ¹é…ï¼Œå› ä¸ºå®é™…ç›®å½•åä¿ç•™äº†ä¸­æ–‡å­—ç¬¦
                logger.info(f"Looking for markdown file for: {pdf_name}")
                
                # ç›´æ¥æœç´¢æ‰€æœ‰temp_å¼€å¤´çš„ç›®å½•
                import glob
                all_temp_dirs = glob.glob(os.path.join(output_dir, "temp_*"))
                logger.info(f"Found all temp directories: {all_temp_dirs}")
                
                # æŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„ç›®å½•
                matching_dirs = []
                for temp_dir in all_temp_dirs:
                    dir_name = os.path.basename(temp_dir)
                    # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ…å«æ–‡ä»¶åï¼ˆå»æ‰æ‰©å±•åï¼‰
                    file_stem = os.path.splitext(pdf_name)[0]
                    # å°†æ–‡ä»¶åä¸­çš„è¿å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿è¿›è¡ŒåŒ¹é…
                    file_stem_normalized = file_stem.replace('-', '_')
                    if file_stem_normalized in dir_name:
                        matching_dirs.append(temp_dir)
                
                logger.info(f"Found matching directories for {pdf_name}: {matching_dirs}")
                
                if matching_dirs:
                    # é€‰æ‹©æœ€æ–°çš„ç›®å½•ï¼ˆæŒ‰æ—¶é—´æˆ³æ’åºï¼‰
                    matching_dirs.sort(reverse=True)
                    parse_dir = matching_dirs[0]
                    logger.info(f"Using directory: {parse_dir}")
                    
                    # æ„å»ºvlmå­ç›®å½•è·¯å¾„
                    if backend.startswith("pipeline"):
                        vlm_dir = os.path.join(parse_dir, parse_method)
                    else:
                        vlm_dir = os.path.join(parse_dir, "vlm")
                    
                    if os.path.exists(vlm_dir):
                        # æŸ¥æ‰¾mdæ–‡ä»¶
                        md_files = glob.glob(os.path.join(vlm_dir, "*.md"))
                        logger.info(f"Found markdown files in {vlm_dir}: {md_files}")
                        
                        if md_files:
                            md_path = md_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªmdæ–‡ä»¶
                            logger.info(f"Using markdown file: {md_path}")
                            
                            if os.path.exists(md_path):
                                with open(md_path, 'r', encoding='utf-8') as f:
                                    txt_content = f.read()
                                # è½¬æ¢å›¾ç‰‡ä¸ºbase64 - ä½¿ç”¨Markdownæ–‡ä»¶æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºç¡€è·¯å¾„
                                md_content = replace_image_with_base64(txt_content, vlm_dir)
                                logger.info(f"Successfully loaded markdown content, length: {len(md_content)}")
                                break
                            else:
                                logger.warning(f"Markdown file does not exist: {md_path}")
                        else:
                            logger.warning(f"No markdown files found in: {vlm_dir}")
                    else:
                        logger.warning(f"VLM directory does not exist: {vlm_dir}")
                else:
                    logger.warning(f"No directories found containing filename: {pdf_name}")
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°Markdownæ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹å†…å®¹
            if not md_content:
                md_content = f"""# {pdf_file_names[0] if pdf_file_names else 'Unknown'}

è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹Markdownæ–‡ä»¶ï¼Œç”±MinerU Webç•Œé¢ç”Ÿæˆã€‚

## æ–‡ä»¶ä¿¡æ¯
- æ–‡ä»¶å: {pdf_file_names[0] if pdf_file_names else 'Unknown'}
- å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- åç«¯: {backend}
- è§£ææ–¹æ³•: {parse_method}

## è¯´æ˜
è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„MinerU Webç•Œé¢ï¼Œç”¨äºæ¼”ç¤ºåŸºæœ¬åŠŸèƒ½ã€‚
è¦ä½¿ç”¨å®Œæ•´çš„PDFè½¬æ¢åŠŸèƒ½ï¼Œè¯·ç¡®ä¿å®‰è£…äº†å®Œæ•´çš„MinerUç¯å¢ƒã€‚

## åŠŸèƒ½ç‰¹æ€§
- å¤šæ–‡ä»¶ä¸Šä¼ 
- æ–‡ä»¶ç±»å‹æ£€æŸ¥
- ZIPæ–‡ä»¶ç”Ÿæˆ
- ä¸­æ–‡ç•Œé¢æ”¯æŒ
"""
                txt_content = md_content
            
            return JSONResponse(content={
                "md_content": md_content,
                "txt_content": txt_content,
                "archive_zip_path": zip_path,
                "new_pdf_path": "",
                "file_name": pdf_file_names[0] if pdf_file_names else "unknown"
            })
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}"}
        )

@app.post("/convert_to_pdf")
async def convert_to_pdf(file: UploadFile = File(...)):
    """å°†éPDFæ–‡ä»¶è½¬æ¢ä¸ºPDFæ ¼å¼"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        file_path = Path(file.filename)
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if file_path.suffix.lower() in pdf_suffixes:
            # å·²ç»æ˜¯PDFæ–‡ä»¶ï¼Œç›´æ¥è¿”å›
            return FileResponse(
                path=None,
                media_type="application/pdf",
                filename=file.filename,
                content=content
            )
        elif file_path.suffix.lower() in image_suffixes:
            # å›¾ç‰‡æ–‡ä»¶ï¼Œä½¿ç”¨to_pdfå‡½æ•°è½¬æ¢
            temp_path = Path("./temp") / f"temp_{file_path.name}"
            temp_path.parent.mkdir(exist_ok=True)
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, "wb") as f:
                f.write(content)
            
            try:
                # ä½¿ç”¨to_pdfå‡½æ•°è½¬æ¢
                pdf_path = to_pdf(str(temp_path))
                
                # è¯»å–è½¬æ¢åçš„PDFæ–‡ä»¶
                with open(pdf_path, "rb") as f:
                    pdf_content = f.read()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_path)
                os.remove(pdf_path)
                
                return FileResponse(
                    path=None,
                    media_type="application/pdf",
                    filename=file_path.stem + ".pdf",
                    content=pdf_content
                )
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_path.exists():
                    os.remove(temp_path)
                raise e
        else:
            return JSONResponse(
                status_code=400,
                content={"error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}"}
            )
            
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"æ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}"}
        )

@app.get("/list_output_files")
async def list_output_files():
    """åˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶"""
    try:
        output_dir = "./output"
        if not os.path.exists(output_dir):
            return JSONResponse(content=[])
        
        files = []
        for item in os.listdir(output_dir):
            item_path = os.path.join(output_dir, item)
            if os.path.isfile(item_path):
                files.append({"name": item, "type": "æ–‡ä»¶"})
            elif os.path.isdir(item_path):
                files.append({"name": item, "type": "ç›®å½•"})
        
        return JSONResponse(content=files)
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}"}
        )

@app.post("/delete_output_files")
async def delete_output_files(request: dict):
    """åˆ é™¤è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶"""
    try:
        output_dir = "./output"
        deleted_files = []
        files = request.get("files", [])
        
        for filename in files:
            file_path = os.path.join(output_dir, filename)
            if os.path.exists(file_path):
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    deleted_files.append(filename)
                elif os.path.isdir(file_path):
                    import shutil
                    shutil.rmtree(file_path)
                    deleted_files.append(filename)
        
        return JSONResponse(content={"deleted": deleted_files})
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"}
        )

@app.get("/download_file/{filename}")
async def download_file(filename: str):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœç›®å½•ï¼ˆZIPæ‰“åŒ…ï¼‰"""
    try:
        output_dir = "./output"
        target_dir = None
        
        # ä¼˜å…ˆç­–ç•¥ï¼šä» file_list.json ä¸­æŸ¥æ‰¾å¯¹åº”çš„ taskIdï¼Œç›´æ¥è®¡ç®—ç›®å½•å
        try:
            file_list = load_server_file_list()
            for file_info in file_list:
                if file_info.get("name") == filename and file_info.get("taskId"):
                    task_id = file_info["taskId"]
                    # è®¡ç®—ç›®å½•åå‰ç¼€ï¼štaskId æ›¿æ¢è¿å­—ç¬¦ä¸ºä¸‹åˆ’çº¿
                    task_id_prefix = task_id.replace('-', '_')
                    
                    # åœ¨ output ç›®å½•ä¸‹æŸ¥æ‰¾ä»¥ taskId_prefix å¼€å¤´çš„ç›®å½•
                    if os.path.exists(output_dir):
                        for item in os.listdir(output_dir):
                            item_path = os.path.join(output_dir, item)
                            if os.path.isdir(item_path) and item.startswith(task_id_prefix):
                                target_dir = item
                                logger.info(f"é€šè¿‡ taskId æ‰¾åˆ°ç›®å½•: {target_dir}")
                                break
                    
                    if target_dir:
                        break
        except Exception as e:
            logger.warning(f"é€šè¿‡ taskId æŸ¥æ‰¾ç›®å½•å¤±è´¥: {e}")
        
        # å¤‡ç”¨ç­–ç•¥ï¼šä½¿ç”¨åŸæ¥çš„æ–‡ä»¶ååŒ¹é…é€»è¾‘
        if not target_dir:
            logger.info(f"ä½¿ç”¨å¤‡ç”¨ç­–ç•¥æŸ¥æ‰¾ç›®å½•: {filename}")
            safe_filename = safe_stem(filename)
            
            # æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•
            matching_dirs = []
            if os.path.exists(output_dir):
                for item in os.listdir(output_dir):
                    item_path = os.path.join(output_dir, item)
                    if os.path.isdir(item_path):
                        # æ£€æŸ¥æ˜¯å¦åŒ¹é… temp_{safe_filename}_{timestamp} æ ¼å¼
                        if item.startswith(f"temp_{safe_filename}_"):
                            matching_dirs.append(item)
                        # ä¹Ÿæ£€æŸ¥æ—§çš„æ ¼å¼ {safe_filename}_{timestamp}ï¼ˆå‘åå…¼å®¹ï¼‰
                        elif item.startswith(f"{safe_filename}_"):
                            matching_dirs.append(item)
            
            if not matching_dirs:
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆå¤„ç†ä¸­æ–‡æ–‡ä»¶åç¼–ç é—®é¢˜ï¼‰
                logger.info(f"æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•å®½æ¾åŒ¹é…æ–‡ä»¶å: {filename}")
                filename_without_ext = Path(filename).stem
                safe_filename_loose = re.sub(r'[^\w\u4e00-\u9fff]', '_', filename_without_ext)  # ä¿ç•™ä¸­æ–‡å­—ç¬¦
                
                for item in os.listdir(output_dir):
                    item_path = os.path.join(output_dir, item)
                    if os.path.isdir(item_path):
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶åçš„ä¸»è¦éƒ¨åˆ†
                        if (f"temp_{safe_filename_loose}_" in item or
                            f"{safe_filename_loose}_" in item):
                            matching_dirs.append(item)
            
            if matching_dirs:
                # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…çš„ç›®å½•ï¼Œé€‰æ‹©æœ€æ–°çš„ï¼ˆæŒ‰æ—¶é—´æˆ³æ’åºï¼‰
                matching_dirs.sort(reverse=True)
                target_dir = matching_dirs[0]
                logger.info(f"é€šè¿‡æ–‡ä»¶ååŒ¹é…æ‰¾åˆ°ç›®å½•: {target_dir}")
        
        if not target_dir:
            return JSONResponse(
                status_code=404,
                content={"error": f"æœªæ‰¾åˆ°æ–‡ä»¶ {filename} çš„å¤„ç†ç»“æœ"}
            )
        
        file_path = os.path.join(output_dir, target_dir)
        
        # åˆ›å»ºZIPæ–‡ä»¶
        zip_path = f"{file_path}.zip"
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(file_path):
                for file in files:
                    file_path_full = os.path.join(root, file)
                    arcname = os.path.relpath(file_path_full, file_path)
                    zipf.write(file_path_full, arcname)
        
        # è¿”å›ZIPæ–‡ä»¶
        safe_filename = safe_stem(filename)
        return FileResponse(
            path=zip_path,
            filename=f"{safe_filename}.zip",
            media_type="application/zip",
            background=BackgroundTask(lambda: os.remove(zip_path))  # ä¸‹è½½ååˆ é™¤ä¸´æ—¶ZIPæ–‡ä»¶
        )
            
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}"}
        )

@app.get("/output/raw/{filename:path}")
async def get_output_file(filename: str):
    """ç›´æ¥ä» ./output ç›®å½•å®‰å…¨åœ°è¿”å›æ–‡ä»¶ï¼ˆç”¨äºPDFé¢„è§ˆï¼‰ã€‚"""
    try:
        base_dir = os.path.abspath("./output")
        # ä»…å…è®¸è®¿é—® output ä¸‹æ–‡ä»¶ï¼Œç¦æ­¢è·¯å¾„ç©¿è¶Š
        requested_path = os.path.abspath(os.path.join(base_dir, filename))
        if not requested_path.startswith(base_dir + os.sep) and requested_path != base_dir:
            return JSONResponse(status_code=403, content={"error": "ç¦æ­¢çš„è·¯å¾„"})

        if not os.path.exists(requested_path) or not os.path.isfile(requested_path):
            return JSONResponse(status_code=404, content={"error": "æ–‡ä»¶ä¸å­˜åœ¨"})

        # ç®€å•çš„å†…å®¹ç±»å‹åˆ¤æ–­
        media_type = "application/pdf" if requested_path.lower().endswith(".pdf") else "application/octet-stream"
        # å¼ºåˆ¶å†…è”æ˜¾ç¤ºï¼Œé¿å…æµè§ˆå™¨ä¸‹è½½ï¼ˆä¸æºå¸¦éASCIIæ–‡ä»¶åï¼Œé¿å…ç¼–ç é—®é¢˜å¯¼è‡´500ï¼‰
        headers = {"Content-Disposition": "inline"}
        return FileResponse(path=requested_path, media_type=media_type, headers=headers)
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"})

@app.get("/download_all")
async def download_all():
    """ä¸‹è½½æ‰€æœ‰å¤„ç†æˆåŠŸçš„æ–‡ä»¶ç›®å½•ï¼ˆZIPæ‰“åŒ…ï¼‰"""
    try:
        output_dir = "./output"
        if not os.path.exists(output_dir):
            return JSONResponse(
                status_code=404,
                content={"error": "è¾“å‡ºç›®å½•ä¸å­˜åœ¨"}
            )
        
        # ä» file_list.json ä¸­è·å–å·²å®Œæˆçš„ä»»åŠ¡
        file_list = load_server_file_list()
        completed_files = []
        
        for file_info in file_list:
            if file_info.get("status") == "completed" and file_info.get("taskId"):
                filename = file_info.get("name")
                task_id = file_info.get("taskId")
                
                # è®¡ç®—ç›®å½•åå‰ç¼€ï¼štaskId æ›¿æ¢è¿å­—ç¬¦ä¸ºä¸‹åˆ’çº¿
                task_id_prefix = task_id.replace('-', '_')
                
                # åœ¨ output ç›®å½•ä¸‹æŸ¥æ‰¾ä»¥ taskId_prefix å¼€å¤´çš„ç›®å½•
                for item in os.listdir(output_dir):
                    item_path = os.path.join(output_dir, item)
                    if os.path.isdir(item_path) and item.startswith(task_id_prefix):
                        # éªŒè¯ç›®å½•ä¸­æ˜¯å¦åŒ…å« .md æ–‡ä»¶ï¼ˆç¡®ä¿å¤„ç†å®Œæˆï¼‰
                        has_md = False
                        for root, _, files in os.walk(item_path):
                            if any(f.lower().endswith('.md') for f in files):
                                has_md = True
                                break
                        
                        if has_md:
                            completed_files.append({
                                "filename": filename,
                                "task_id": task_id,
                                "directory": item,
                                "path": item_path
                            })
                            logger.info(f"æ‰¾åˆ°å·²å®Œæˆæ–‡ä»¶: {filename} -> {item}")
                        break

        if not completed_files:
            return JSONResponse(
                status_code=404,
                content={"error": "æ²¡æœ‰å¯ä¸‹è½½çš„å·²å®Œæˆæ–‡ä»¶"}
            )
        
        # å½’æ¡£åï¼šall_results_{æ—¶é—´æˆ³}.zip
        timestamp = time.strftime("%y%m%d_%H%M%S")
        zip_filename = f"all_results_{timestamp}.zip"
        zip_path = os.path.join(output_dir, zip_filename)
        
        # åˆ›å»ºZIPï¼Œä¿æŒå®Œæ•´ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹ output æ ¹ï¼‰
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_info in completed_files:
                dir_path = file_info["path"]
                for root, _, files in os.walk(dir_path):
                    for file in files:
                        file_path_full = os.path.join(root, file)
                        arcname = os.path.relpath(file_path_full, output_dir)
                        zipf.write(file_path_full, arcname)
        
        logger.info(f"æˆåŠŸæ‰“åŒ… {len(completed_files)} ä¸ªå·²å®Œæˆæ–‡ä»¶")
        return FileResponse(
            path=zip_path,
            filename=zip_filename,
            media_type="application/zip",
            background=BackgroundTask(lambda: os.remove(zip_path))
        )
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"ä¸‹è½½æ‰€æœ‰æ–‡ä»¶å¤±è´¥: {str(e)}"}
        )

@app.post("/download_all")
async def download_all_selected(request: dict):
    """æŒ‰æœ¬æ¬¡ä»»åŠ¡æä¾›çš„æ–‡ä»¶åˆ—è¡¨æ‰“åŒ…ä¸‹è½½ï¼ˆä»…æˆåŠŸç›®å½•ï¼‰ã€‚
    è¯·æ±‚ä½“ç¤ºä¾‹: {"files": ["a.pdf", "b.pdf"]}
    """
    try:
        output_dir = "./output"
        if not os.path.exists(output_dir):
            return JSONResponse(status_code=404, content={"error": "è¾“å‡ºç›®å½•ä¸å­˜åœ¨"})

        file_names = request.get("files", []) or []
        if not isinstance(file_names, list) or not file_names:
            return JSONResponse(status_code=400, content={"error": "ç¼ºå°‘å¾…æ‰“åŒ…æ–‡ä»¶åˆ—è¡¨"})

        # ç›´æ¥ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ–‡ä»¶ï¼Œä¸æ£€æµ‹çŠ¶æ€
        # æ³¨é‡Šæ‰çŠ¶æ€æ£€æµ‹é€»è¾‘ï¼Œå…è®¸ä¸‹è½½ä»»ä½•é€‰æ‹©çš„æ–‡ä»¶

        # ç›´æ¥åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾ç”¨æˆ·é€‰æ‹©çš„æ–‡ä»¶å¯¹åº”çš„ç›®å½•
        selected_dirs = []
        
        # æ–¹æ³•1: ç›´æ¥åŒ¹é…æ–‡ä»¶å
        if os.path.exists(output_dir):
            for item_name in os.listdir(output_dir):
                item_path = os.path.join(output_dir, item_name)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ…å«ç”¨æˆ·é€‰æ‹©çš„æ–‡ä»¶å
                    for filename in file_names:
                        # ç§»é™¤æ–‡ä»¶æ‰©å±•åè¿›è¡ŒåŒ¹é…
                        file_stem = Path(filename).stem
                        if (item_name == filename or 
                            file_stem in item_name or 
                            item_name.startswith(file_stem)):
                            selected_dirs.append(item_name)
                            logger.info(f"ç›´æ¥åŒ¹é…æ‰¾åˆ°ç›®å½•: {item_name} (å¯¹åº”æ–‡ä»¶: {filename})")
                            break
        
        # æ–¹æ³•2: é€šè¿‡file_list.jsonæŸ¥æ‰¾å¯¹åº”çš„taskIdç›®å½•ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
        if not selected_dirs:
            try:
                server_list = load_server_file_list()
                for filename in file_names:
                    for item in server_list:
                        if item.get("name") == filename:
                            task_id = item.get("taskId") or item.get("task_id")
                            if task_id:
                                task_id_prefix = task_id.replace('-', '_')
                                if os.path.exists(output_dir):
                                    for item_name in os.listdir(output_dir):
                                        item_path = os.path.join(output_dir, item_name)
                                        if (os.path.isdir(item_path) and 
                                            item_name.startswith(task_id_prefix)):
                                            selected_dirs.append(item_name)
                                            logger.info(f"é€šè¿‡taskIdæ‰¾åˆ°ç›®å½•: {item_name} (å¯¹åº”æ–‡ä»¶: {filename})")
                                            break
                            break
            except Exception as e:
                logger.warning(f"é€šè¿‡file_list.jsonæŸ¥æ‰¾ç›®å½•å¤±è´¥: {e}")

        if not selected_dirs:
            return JSONResponse(status_code=404, content={"error": "æ²¡æœ‰å¯ä¸‹è½½çš„ç›®å½•"})

        timestamp = time.strftime("%y%m%d_%H%M%S")
        zip_filename = f"all_results_{timestamp}.zip"
        zip_path = os.path.join(output_dir, zip_filename)

        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            total_dirs = len(selected_dirs)
            for i, directory in enumerate(selected_dirs):
                logger.info(f"æ­£åœ¨æ‰“åŒ…ç›®å½• {i+1}/{total_dirs}: {directory}")
                dir_path = os.path.join(output_dir, directory)
                for root, _, files in os.walk(dir_path):
                    for file in files:
                        file_path_full = os.path.join(root, file)
                        arcname = os.path.relpath(file_path_full, output_dir)
                        zipf.write(file_path_full, arcname)
                logger.info(f"å·²æ‰“åŒ…ç›®å½• {i+1}/{total_dirs}: {directory}")

        return FileResponse(
            path=zip_path,
            filename=zip_filename,
            media_type="application/zip",
            background=BackgroundTask(lambda: os.remove(zip_path))
        )

    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"ä¸‹è½½æ‰€æœ‰æ–‡ä»¶å¤±è´¥: {str(e)}"})

@app.get("/output/find_pdf")
async def find_pdf(q: str):
    """æ ¹æ®å…³é”®è¯ï¼ˆåŸå§‹æ–‡ä»¶åæˆ–ä»»åŠ¡ç›®å½•åï¼‰åœ¨ ./output ä¸‹å¯»æ‰¾å¯é¢„è§ˆçš„ PDFã€‚
    ä¼˜å…ˆä½¿ç”¨ file_list.json ä¸­çš„ taskId è®¡ç®—ç›®å½•åï¼ŒæŸ¥æ‰¾ ç›®å½•å/auto/ç›®å½•å+_origin.pdfã€‚
    æ‰¾ä¸åˆ°å†ä½¿ç”¨åŸæ¥çš„å…³é”®è¯åŒ¹é…é€»è¾‘ã€‚
    è¿”å›ç›¸å¯¹ ./output çš„è·¯å¾„ï¼Œç”¨äº /output/raw/{path} è®¿é—®ã€‚
    """
    try:
        base_dir = os.path.abspath("./output")
        if not os.path.exists(base_dir):
            return JSONResponse(status_code=404, content={"error": "è¾“å‡ºç›®å½•ä¸å­˜åœ¨"})

        keyword = q or ""
        
        # ä¼˜å…ˆç­–ç•¥ï¼šä» file_list.json ä¸­æŸ¥æ‰¾å¯¹åº”çš„ taskIdï¼Œé€šè¿‡å‰ç¼€åŒ¹é…æ‰¾åˆ°ç›®å½•
        try:
            file_list = load_server_file_list()
            for file_info in file_list:
                if file_info.get("name") == keyword and file_info.get("taskId"):
                    task_id = file_info["taskId"]
                    # è®¡ç®—ç›®å½•åå‰ç¼€ï¼štaskId æ›¿æ¢è¿å­—ç¬¦ä¸ºä¸‹åˆ’çº¿
                    task_id_prefix = task_id.replace('-', '_')
                    
                    # åœ¨ output ç›®å½•ä¸‹æŸ¥æ‰¾ä»¥ taskId_prefix å¼€å¤´çš„ç›®å½•
                    for item in os.listdir(base_dir):
                        item_path = os.path.join(base_dir, item)
                        if os.path.isdir(item_path) and item.startswith(task_id_prefix):
                            # æ„é€ é¢„æœŸçš„PDFè·¯å¾„ï¼šç›®å½•å/auto/ç›®å½•å+_origin.pdf
                            expected_pdf_path = os.path.join(item, "auto", f"{item}_origin.pdf")
                            full_expected_path = os.path.join(base_dir, expected_pdf_path)
                            
                            if os.path.exists(full_expected_path) and os.path.isfile(full_expected_path):
                                logger.info(f"é€šè¿‡ taskId æ‰¾åˆ°PDF: {expected_pdf_path}")
                                return JSONResponse(content={"path": expected_pdf_path})
        except Exception as e:
            logger.warning(f"é€šè¿‡ taskId æŸ¥æ‰¾PDFå¤±è´¥: {e}")
        
        # å¤‡ç”¨ç­–ç•¥ï¼šä½¿ç”¨åŸæ¥çš„å…³é”®è¯åŒ¹é…é€»è¾‘
        # åŒæ—¶å°è¯•åŸå§‹ã€safe_stemã€è¿å­—ç¬¦æ›¿æ¢
        candidates = list({
            keyword,
            safe_stem(keyword),
            Path(keyword).stem,
            safe_stem(Path(keyword).stem),
            (Path(keyword).stem.replace('-', '_') if keyword else "")
        } - {""})

        hit_origin = None
        hit_any = None

        for root, dirs, files in os.walk(base_dir):
            # ä»…åœ¨ auto æˆ– vlm å­ç›®å½•é‡Œæ‰¾
            if os.path.basename(root) not in ("auto", "vlm"):
                continue
            rel_dir = os.path.relpath(root, base_dir)
            for file in files:
                if not file.lower().endswith('.pdf'):
                    continue
                rel_path = os.path.join(rel_dir, file)
                full_path_lower = rel_path.lower()
                # å…³é”®è¯åŒ¹é…
                if candidates and not any(c.lower() in full_path_lower for c in candidates):
                    continue
                if file.endswith("_origin.pdf") and hit_origin is None:
                    hit_origin = rel_path
                if hit_any is None:
                    hit_any = rel_path
            # æå‰ç»“æŸï¼šæ‰¾åˆ°ä¼˜å…ˆæ–‡ä»¶
            if hit_origin:
                break

        chosen = hit_origin or hit_any
        if not chosen:
            return JSONResponse(status_code=404, content={"error": "æœªæ‰¾åˆ°åŒ¹é…çš„PDF"})

        return JSONResponse(content={"path": chosen})
    except Exception as e:
        logger.exception(e)
        return JSONResponse(status_code=500, content={"error": f"æŸ¥æ‰¾å¤±è´¥: {str(e)}"})

# æ–°å¢çš„ä»»åŠ¡ç®¡ç†APIç«¯ç‚¹
@app.post("/api/upload_with_progress")
async def upload_with_progress(files: List[UploadFile] = File(...)):
    """ä¸Šä¼ æ–‡ä»¶å¹¶åˆ›å»ºåå°å¤„ç†ä»»åŠ¡ï¼Œæ”¯æŒè¿›åº¦æ¡"""
    try:
        task_ids = []
        
        for file in files:
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            file_path = Path(file.filename)
            if file_path.suffix.lower() not in pdf_suffixes + image_suffixes:
                return JSONResponse(
                    status_code=400,
                    content={"error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}"}
                )
            
            # åˆ›å»ºä»»åŠ¡
            task_id = task_manager.create_task(file.filename)
            
            # ä¿å­˜æ–‡ä»¶åˆ°outputç›®å½•
            output_path = os.path.join("./output", f"{task_id}_{file.filename}")
            _ensure_output_dir()
            
            content = await file.read()
            with open(output_path, "wb") as f:
                f.write(content)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²ä¸Šä¼ 
            task_manager.update_task_status(task_id, TaskStatus.PENDING, 10, "æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
            
            # è‡ªåŠ¨åŠ å…¥é˜Ÿåˆ—
            task_manager.add_to_queue(task_id)
            
            task_ids.append(task_id)
            
        return JSONResponse(content={
            "task_ids": task_ids,
            "queue_status": task_manager.queue_status.value,
            "message": f"æˆåŠŸä¸Šä¼  {len(task_ids)} ä¸ªæ–‡ä»¶ï¼Œå·²è‡ªåŠ¨åŠ å…¥é˜Ÿåˆ—"
        })
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}"}
        )

@app.get("/api/tasks")
async def get_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    try:
        return JSONResponse(content=task_manager.get_all_tasks())
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}"}
        )

@app.get("/api/task/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡çš„çŠ¶æ€"""
    try:
        task = task_manager.get_task(task_id)
        if not task:
            return JSONResponse(
                status_code=404,
                content={"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
            )
        return JSONResponse(content=task.to_dict())
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}"}
        )

@app.get("/api/task/{task_id}/markdown")
async def get_task_markdown(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡çš„Markdownå†…å®¹"""
    try:
        # ä¼˜å…ˆä»ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡ä¿¡æ¯
        task = task_manager.get_task(task_id)
        task_info = None
        
        if task:
            task_info = {
                "filename": task.filename,
                "status": task.status,
                "result_path": task.result_path
            }
        else:
            # å¦‚æœä»»åŠ¡ç®¡ç†å™¨ä¸­æ‰¾ä¸åˆ°ï¼Œä» file_list.json ä¸­æŸ¥æ‰¾
            file_list = load_server_file_list()
            for file_info in file_list:
                if file_info.get("taskId") == task_id:
                    # æ„é€ ä»»åŠ¡ä¿¡æ¯
                    task_info = {
                        "filename": file_info.get("name"),
                        "status": TaskStatus.COMPLETED if file_info.get("status") == "completed" else TaskStatus.PENDING,
                        "result_path": None  # éœ€è¦æ ¹æ®ç›®å½•ç»“æ„è®¡ç®—
                    }
                    break
        
        if not task_info:
            return JSONResponse(
                status_code=404,
                content={"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
            )
        
        if task_info["status"] != TaskStatus.COMPLETED:
            return JSONResponse(
                status_code=400,
                content={"error": "ä»»åŠ¡å°šæœªå®Œæˆ"}
            )
        
        # å¦‚æœæ²¡æœ‰ result_pathï¼Œéœ€è¦æ ¹æ® taskId è®¡ç®—
        result_path = task_info["result_path"]
        if not result_path:
            # è®¡ç®—è¾“å‡ºç›®å½•è·¯å¾„
            task_id_prefix = task_id.replace('-', '_')
            base_dir = os.path.abspath("./output")
            
            # æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•
            for item in os.listdir(base_dir):
                item_path = os.path.join(base_dir, item)
                if os.path.isdir(item_path) and item.startswith(task_id_prefix):
                    result_path = item_path
                    break
        
        # è·å–Markdownå†…å®¹
        md_content, txt_content = await load_task_markdown_content(task_info["filename"], result_path)
        
        return JSONResponse(content={
            "task_id": task_id,
            "filename": task_info["filename"],
            "md_content": md_content,
            "txt_content": txt_content,
            "status": task_info["status"].value
        })
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"è·å–Markdownå†…å®¹å¤±è´¥: {str(e)}"}
        )

@app.post("/api/start_background_processing")
async def start_background_processing(task_ids: List[str] = Form(...)):
    """å¯åŠ¨åå°å¤„ç†ä»»åŠ¡"""
    try:
        # å¯åŠ¨åå°å¤„ç†
        asyncio.create_task(process_tasks_background(task_ids))
        
        return JSONResponse(content={
            "message": f"å·²å¯åŠ¨ {len(task_ids)} ä¸ªä»»åŠ¡çš„åå°å¤„ç†ï¼Œæ‚¨å¯ä»¥å…³é—­æµè§ˆå™¨"
        })
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"å¯åŠ¨åå°å¤„ç†å¤±è´¥: {str(e)}"}
        )

@app.post("/api/queue/start")
async def start_queue():
    """å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—"""
    try:
        task_manager.start_queue()
        # å¼€å§‹å¤„ç†é˜Ÿåˆ—
        asyncio.create_task(task_manager.process_queue())
        
        return JSONResponse(content={
            "message": "ä»»åŠ¡é˜Ÿåˆ—å·²å¯åŠ¨",
            "queue_status": task_manager.queue_status.value
        })
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"å¯åŠ¨é˜Ÿåˆ—å¤±è´¥: {str(e)}"}
        )

@app.post("/api/queue/stop")
async def stop_queue():
    """åœæ­¢ä»»åŠ¡é˜Ÿåˆ—"""
    try:
        task_manager.stop_queue()
        
        return JSONResponse(content={
            "message": "ä»»åŠ¡é˜Ÿåˆ—å·²åœæ­¢",
            "queue_status": task_manager.queue_status.value
        })
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"åœæ­¢é˜Ÿåˆ—å¤±è´¥: {str(e)}"}
        )

@app.get("/api/queue/status")
async def get_queue_status():
    """è·å–é˜Ÿåˆ—çŠ¶æ€"""
    try:
        queued_tasks = task_manager.get_queue_tasks()
        
        return JSONResponse(content={
            "queue_status": task_manager.queue_status.value,
            "current_processing_task": task_manager.current_processing_task,
            "queued_tasks": queued_tasks,
            "queued_count": len(queued_tasks)
        })
        
    except Exception as e:
        logger.exception(e)
        return JSONResponse(
            status_code=500,
            content={"error": f"è·å–é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {str(e)}"}
        )

async def process_tasks_background(task_ids: List[str]):
    """åå°å¤„ç†ä»»åŠ¡"""
    for task_id in task_ids:
        try:
            task = task_manager.get_task(task_id)
            if not task:
                continue
                
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING, 20, "å¼€å§‹å¤„ç†æ–‡ä»¶")
            
            # æŸ¥æ‰¾ä¸Šä¼ çš„æ–‡ä»¶
            output_dir = "./output"
            uploaded_file = None
            for filename in os.listdir(output_dir):
                if filename.startswith(f"{task_id}_"):
                    uploaded_file = os.path.join(output_dir, filename)
                    break
                    
            if not uploaded_file:
                task_manager.update_task_status(task_id, TaskStatus.FAILED, 0, "æ‰¾ä¸åˆ°ä¸Šä¼ çš„æ–‡ä»¶", "æ–‡ä»¶ä¸å­˜åœ¨")
                continue
                
            # å¼€å§‹å¤„ç†
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING, 30, "æ­£åœ¨è§£ææ–‡ä»¶")
            
            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
            async def update_progress(progress, message):
                task_manager.update_task_status(task_id, TaskStatus.PROCESSING, progress, message)
                # æ·»åŠ æ—¥å¿—è®°å½•è¿›åº¦æ›´æ–°
                logger.info(f"ä»»åŠ¡ {task_id} è¿›åº¦æ›´æ–°: {progress}% - {message}")
            
            # ä½¿ç”¨ç°æœ‰çš„parse_pdfå‡½æ•°è¿›è¡Œå¤„ç†
            result = await parse_pdf(
                doc_path=uploaded_file,
                output_dir=output_dir,
                end_page_id=99999,
                is_ocr=False,
                formula_enable=True,
                table_enable=True,
                language="ch",
                backend="vlm-sglang-engine",
                url=None,
                progress_callback=update_progress
            )
            
            if result:
                local_md_dir, file_name = result
                task_manager.update_task_status(task_id, TaskStatus.PROCESSING, 80, "å¤„ç†å®Œæˆï¼Œç”Ÿæˆç»“æœæ–‡ä»¶")
                
                # ä¿å­˜ç»“æœè·¯å¾„
                task.result_path = local_md_dir
                task_manager.update_task_status(task_id, TaskStatus.COMPLETED, 100, "è½¬æ¢å®Œæˆ", None)
                
                # è¾“å‡ºoutputç›®å½•ä¿¡æ¯
                print(f"âœ… æ–‡ä»¶è½¬æ¢æˆåŠŸ: {file_name}")
                print(f"ğŸ“ è¾“å‡ºç›®å½•: {local_md_dir}")
                logger.info(f"ä»»åŠ¡ {task_id} å¤„ç†å®Œæˆ: {file_name}")
                logger.info(f"è¾“å‡ºç›®å½•: {local_md_dir}")
                
                # æ¸…ç†ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶
                try:
                    os.remove(uploaded_file)
                except:
                    pass
                    
            else:
                task_manager.update_task_status(task_id, TaskStatus.FAILED, 0, "å¤„ç†å¤±è´¥", "è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
                
        except Exception as e:
            logger.exception(f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‡ºé”™: {e}")
            task_manager.update_task_status(task_id, TaskStatus.FAILED, 0, "å¤„ç†å¤±è´¥", str(e))

@click.command(context_settings=dict(ignore_unknown_options=True, allow_extra_args=True))
@click.pass_context
@click.option(
    '--enable-sglang-engine',
    'sglang_engine_enable',
    type=bool,
    help="å¯ç”¨SgLangå¼•æ“åç«¯ä»¥åŠ å¿«å¤„ç†é€Ÿåº¦",
    default=True,
)
@click.option(
    '--max-convert-pages',
    'max_convert_pages',
    type=int,
    help="è®¾ç½®ä»PDFè½¬æ¢ä¸ºMarkdownçš„æœ€å¤§é¡µæ•°",
    default=1000,
)
@click.option(
    '--host',
    'host',
    type=str,
    help="è®¾ç½®æœåŠ¡å™¨ä¸»æœºå",
    default='0.0.0.0',
)
@click.option(
    '--port',
    'port',
    type=int,
    help="è®¾ç½®æœåŠ¡å™¨ç«¯å£",
    default=7860,
)
def main(ctx, sglang_engine_enable, max_convert_pages, host, port, **kwargs):
    """å¯åŠ¨MinerU Webç•Œé¢"""
    kwargs.update(arg_parse(ctx))
    
    # å°†é…ç½®å‚æ•°å­˜å‚¨åˆ°åº”ç”¨çŠ¶æ€ä¸­
    app.state.config = kwargs
    app.state.max_convert_pages = max_convert_pages
    app.state.sglang_engine_enable = sglang_engine_enable
    
    if sglang_engine_enable and MINERU_AVAILABLE:
        try:
            print("æ­£åœ¨åˆå§‹åŒ–SgLangå¼•æ“...")
            from mineru.backend.vlm.vlm_analyze import ModelSingleton
            model_singleton = ModelSingleton()
            
            # è¿‡æ»¤æ‰ä¸åº”è¯¥ä¼ é€’ç»™SgLangå¼•æ“çš„å‚æ•°
            sglang_kwargs = {k: v for k, v in kwargs.items() 
                           if k not in ['server_name', 'server_port', 'host', 'port', 'enable_api', 'api_enable']}
            
            predictor = model_singleton.get_model(
                "sglang-engine",
                None,
                None,
                **sglang_kwargs
            )
            print("SgLangå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.exception(e)
    
    print(f"å¯åŠ¨MinerU Webç•Œé¢: http://{host}:{port}")
    print("ç•Œé¢åŠŸèƒ½:")
    print("- å¤šæ–‡ä»¶æ‹–æ‹½ä¸Šä¼ ")
    print("- å®æ—¶è½¬æ¢çŠ¶æ€æ˜¾ç¤º")
    print("- ç»“æœæ–‡ä»¶ä¸‹è½½")
    print("- è¾“å‡ºç›®å½•ç®¡ç†")
    
    if not MINERU_AVAILABLE:
        print("æ³¨æ„: ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ŒMinerUæ¨¡å—ä¸å¯ç”¨")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        reload=False
    )

if __name__ == '__main__':
    main()
